{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e8e41cb",
   "metadata": {},
   "source": [
    "###### PySpark RDD – Resilient Distributed Dataset\n",
    "In this section of the PySpark tutorial, I will introduce the RDD and explains how to create them, and use its transformation and action operations with examples. Here is the full article on PySpark RDD in case if you wanted to learn more of and get your fundamentals strong.\n",
    "\n",
    "###### PySpark RDD (Resilient Distributed Dataset)\n",
    "A fundamental data structure of PySpark that is fault-tolerant, immutable distributed collections of objects, which means once you create an RDD you cannot change it. Each dataset in RDD is divided into logical partitions, which can be computed on different nodes of the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e86cd00",
   "metadata": {},
   "source": [
    "###### RDD Creation \n",
    "In order to create an RDD, first, you need to create a SparkSession which is an entry point to the PySpark application. SparkSession can be created using a ```builder()``` or ```newSession()``` methods of the SparkSession.\n",
    "\n",
    "Spark session internally creates a sparkContext variable of SparkContext. You can create multiple SparkSession objects but only one SparkContext per JVM. In case if you want to create another new SparkContext you should stop existing Sparkcontext (using stop()) before creating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdd43f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up _JAVA_OPTIONS: -Dawt.useSystemAAFontSettings=on -Dswing.aatext=true\n",
      "Picked up _JAVA_OPTIONS: -Dawt.useSystemAAFontSettings=on -Dswing.aatext=true\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/03 07:39:56 WARN Utils: Your hostname, Jkop resolves to a loopback address: 127.0.1.1; using 172.30.94.201 instead (on interface eth0)\n",
      "22/09/03 07:39:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/03 07:39:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[*]\") \\\n",
    "      .appName(\"SparkByExamples.com\") \\\n",
    "      .getOrCreate() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaae0f2",
   "metadata": {},
   "source": [
    "###### using parallelize()\n",
    "SparkContext has several functions to use with RDDs. For example, it’s ```parallelize()``` method is used to create an RDD from a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "326a1fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD from parallelize    \n",
    "dataList = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "rdd=spark.sparkContext.parallelize(dataList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137f1b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Java', 20000), ('Python', 100000), ('Scala', 3000)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caa5a86",
   "metadata": {},
   "source": [
    "###### using textFile()\n",
    "RDD can also be created from a text file using ```textFile()``` function of the SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c73ddcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD from external Data source\n",
    "rdd2 = spark.sparkContext.textFile(\"/path/test.txt\")\n",
    "\n",
    "#Once you have an RDD, you can perform transformation and action operations. Any operation you perform on RDD runs in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17858757",
   "metadata": {},
   "source": [
    "RDD Operations - On PySpark RDD, you can perform two kinds of operations.\n",
    "\n",
    "###### RDD transformations\n",
    "Transformations are lazy operations. When you run a transformation(for example update), instead of updating a current RDD, these operations return another RDD.\n",
    "\n",
    "Transformations on Spark RDD returns another RDD and transformations are lazy meaning they don’t execute until you call an action on RDD. Some transformations on RDD’s are ```flatMap(),``` ```map(),``` ```reduceByKey(),``` ```filter(),``` ```sortByKey()``` and return new RDD instead of updating the current.\n",
    "\n",
    "##### RDD actions \n",
    "Operations that trigger computation and return RDD values to the driver.\n",
    "\n",
    "RDD Action operation returns the values from an RDD to a driver node. In other words, any RDD function that returns non RDD[T] is considered as an action. \n",
    "\n",
    "Some actions on RDDs are ```count(),``` ```collect(),``` ```first(),``` ```max(),``` ```reduce()``` and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af0cf27",
   "metadata": {},
   "source": [
    "###### PySpark DataFrame\n",
    "DataFrame definition is very well explained by Databricks hence I do not want to define it again and confuse you. Below is the definition I took it from Databricks.\n",
    "\n",
    "DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as structured data files, tables in Hive, external databases, or existing RDDs.\n",
    "\n",
    "– Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e8ef1e",
   "metadata": {},
   "source": [
    "###### DataFrame creation\n",
    "The simplest way to create a DataFrame is from a Python list of data. DataFrame can also be created from an RDD and by reading files from several sources using ```createDataFrame().```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fd38c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5b8416e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe4a951",
   "metadata": {},
   "source": [
    "###### DataFrame operations\n",
    "Like RDD, DataFrame also has operations like Transformations and Actions.\n",
    "\n",
    "###### DataFrame from external data sources\n",
    "In real-time applications, DataFrames are created from external sources like files from the local system, HDFS, S3 Azure, HBase, MySQL table e.t.c. Below is an example of how to read a CSV file from a local system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "528681d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.csv(\"/tmp/resources/zipcodes.csv\")\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa1d71",
   "metadata": {},
   "source": [
    "###### Supported file formats\n",
    "DataFrame has a rich set of API which supports reading and writing several file formats\n",
    "* csv\n",
    "* text\n",
    "* Avro\n",
    "* Parquet\n",
    "* tsv\n",
    "* xml and many more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c0f03f",
   "metadata": {},
   "source": [
    "###### PySpark SQL Tutorial\n",
    "```PySpark SQL is one of the most used PySpark modules which is used for processing structured columnar data format. Once you have a DataFrame created, you can interact with the data by using SQL syntax.\n",
    "\n",
    "In other words, Spark SQL brings native RAW SQL queries on Spark meaning you can run traditional ANSI SQL’s on Spark Dataframe, in the later section of this PySpark SQL tutorial, you will learn in detail using SQL select, where, group by, join, union e.t.c\n",
    "\n",
    "In order to use SQL, first, create a temporary table on DataFrame using createOrReplaceTempView() function. Once created, this table can be accessed throughout the SparkSession using sql() and it will be dropped along with your SparkContext termination.\n",
    "\n",
    "Use sql() method of the SparkSession object to run the query and this method returns a new DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ce5b4",
   "metadata": {},
   "source": [
    "###### SparkSession \n",
    "SparkSession was introduced in version 2.0, It is an entry point to underlying PySpark functionality in order to programmatically create PySpark RDD, DataFrame. It’s object spark is default available in pyspark-shell and it can be created programmatically using SparkSession."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69a8ae9",
   "metadata": {},
   "source": [
    "###### Create SparkSession\n",
    "\n",
    "In order to create SparkSession programmatically (in .py file) in PySpark, you need to use the builder pattern method ```builder()``` as explained below. ```getOrCreate()``` method returns an already existing SparkSession; if not exists, it creates a new SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99566e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession from builder\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1300e1",
   "metadata": {},
   "source": [
    "```master()``` – If you are running it on the cluster you need to use your master name as an argument to master(). usually, it would be either ```yarn``` or ```mesos``` depends on your cluster setup.\n",
    "\n",
    "   * Use ```local[x]``` when running in Standalone mode. x should be an integer value and should be greater than 0; this represents how many partitions it should create when using RDD, DataFrame, and Dataset. Ideally, x value should be the number of CPU cores you have.\n",
    "\n",
    "```appName()``` – Used to set your application name.\n",
    "\n",
    "```getOrCreate()``` – This returns a SparkSession object if already exists, and creates a new one if not exist.\n",
    "\n",
    "Note:  SparkSession object ```spark``` is by default available in the PySpark shell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39904cf9",
   "metadata": {},
   "source": [
    "###### Create Another SparkSession\n",
    "\n",
    "Create a new SparkSession using ```newSession()``` method. This uses the same app name, master as the existing session. Underlying SparkContext will be the same for both sessions as you can have only one context per PySpark application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9b424dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function SparkSession.newSession at 0x7ff6e0769ee0>\n"
     ]
    }
   ],
   "source": [
    "# Create new SparkSession\n",
    "spark2 = SparkSession.newSession\n",
    "print(spark2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb91a853",
   "metadata": {},
   "source": [
    "###### Get Existing SparkSession\n",
    "\n",
    "You can get the existing SparkSession in PySpark using the ```builder.getOrCreate(),``` for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fa86b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method SparkSession.Builder.getOrCreate of <pyspark.sql.session.SparkSession.Builder object at 0x7ff6e087d610>>\n"
     ]
    }
   ],
   "source": [
    "# Get Existing SparkSession\n",
    "spark3 = SparkSession.builder.getOrCreate\n",
    "print(spark3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f489ec12",
   "metadata": {},
   "source": [
    "###### Using Spark Config\n",
    "\n",
    "If you wanted to set some configs to SparkSession, use the ```config()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cb3f200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/03 08:02:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Usage of config()\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"SparkByExamples.com\") \\\n",
    "      .config(\"spark.some.config.option\", \"config-value\") \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c33acb",
   "metadata": {},
   "source": [
    "###### SparkSession Commonly Used Methods\n",
    "\n",
    "```version()``` – Returns the Spark version where your application is running, probably the Spark version your cluster is configured with.\n",
    "\n",
    "```createDataFrame()``` – This creates a DataFrame from a collection and an RDD\n",
    "\n",
    "```getActiveSession()``` – returns an active Spark session.\n",
    "\n",
    "```read()``` – Returns an instance of ```DataFrameReader``` class, this is used to read records from csv, parquet, avro, and more file formats into DataFrame.\n",
    "\n",
    "```readStream()``` – Returns an instance of ```DataStreamReader``` class, this is used to read streaming data. that can be used to read streaming data into DataFrame.\n",
    "\n",
    "```sparkContext()``` – Returns a ```SparkContext.```\n",
    "\n",
    "```sql()``` – Returns a DataFrame after executing the SQL mentioned.\n",
    "\n",
    "```sqlContext()``` – Returns SQLContext.\n",
    "\n",
    "```stop()``` – Stop the current SparkContext.\n",
    "\n",
    "```table()``` – Returns a DataFrame of a table or view.\n",
    "\n",
    "```udf()``` – Creates a PySpark UDF to use it on DataFrame, Dataset, and SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a5a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
