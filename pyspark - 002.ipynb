{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24cc8720",
   "metadata": {},
   "source": [
    "###### What is RDD (Resilient Distributed Dataset)?\n",
    "\n",
    "RDD (Resilient Distributed Dataset) is a fundamental building block of PySpark which is fault-tolerant, immutable distributed collections of objects. Immutable meaning once you create an RDD you cannot change it. Each record in RDD is divided into logical partitions, which can be computed on different nodes of the cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f269488a",
   "metadata": {},
   "source": [
    "In other words, RDDs are a collection of objects similar to list in Python, with the difference being RDD is computed on several processes scattered across multiple physical servers also called nodes in a cluster while a Python collection lives and process in just one process.\n",
    "\n",
    "Additionally, RDDs provide data abstraction of partitioning and distribution of the data designed to run computations in parallel on several nodes, while doing transformations on RDD we don’t have to worry about the parallelism as PySpark by default provides.\n",
    "\n",
    "This Apache PySpark RDD tutorial describes the basic operations available on RDDs, such as map(), filter(), and persist() and many more. In addition, this tutorial also explains Pair RDD functions that operate on RDDs of key-value pairs such as groupByKey() and join() etc.\n",
    "\n",
    "Note: RDD’s can have a name and unique identifier (id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1451a0",
   "metadata": {},
   "source": [
    "###### Creating RDD\n",
    "\n",
    "RDD’s are created primarily in two different ways,\n",
    "\n",
    "   * parallelizing an existing collection and\n",
    "   * referencing a dataset in an external storage system (```HDFS```, ```S3``` and many more). \n",
    "\n",
    "Before we look into examples, first let’s initialize SparkSession using the builder pattern method defined in SparkSession class. While initializing, we need to provide the master and application name as shown below. In realtime application, you will pass master from spark-submit instead of hardcoding on Spark application.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a65902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up _JAVA_OPTIONS: -Dawt.useSystemAAFontSettings=on -Dswing.aatext=true\n",
      "Picked up _JAVA_OPTIONS: -Dawt.useSystemAAFontSettings=on -Dswing.aatext=true\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/04 06:27:50 WARN Utils: Your hostname, Jkop resolves to a loopback address: 127.0.1.1; using 172.30.92.24 instead (on interface eth0)\n",
      "22/09/04 06:27:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/04 06:27:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark:SparkSession = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"SparkByExamples.com\") \\\n",
    "      .getOrCreate()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe73f23",
   "metadata": {},
   "source": [
    "```master()``` – If you are running it on the cluster you need to use your master name as an argument to master(). usually, it would be either yarn (Yet Another Resource Negotiator) or mesos depends on your cluster setup.\n",
    "\n",
    "  * Use local[x] when running in Standalone mode. x should be an integer value and should be greater than 0; this represents how many partitions it should create when using RDD, DataFrame, and Dataset. Ideally, x value should be the number of CPU cores you have.\n",
    "\n",
    "```appName()``` – Used to set your application name.\n",
    "\n",
    "```getOrCreate()``` – This returns a SparkSession object if already exists, and creates a new one if not exist.\n",
    "\n",
    "Note: Creating SparkSession object, internally creates one SparkContext per JVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c5f60",
   "metadata": {},
   "source": [
    "###### Create RDD using sparkContext.parallelize()\n",
    "\n",
    "By using ```parallelize()``` function of SparkContext (sparkContext.parallelize() ) you can create an RDD. This function loads the existing collection from your driver program into parallelizing RDD. This is a basic method to create RDD and is used when you already have data in memory that is either loaded from a file or from a database. and it required all data to be present on the driver program prior to creating RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd4eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create RDD from parallelize    \n",
    "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rdd=spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee9050",
   "metadata": {},
   "source": [
    "For production applications, we mostly create RDD by using external storage systems like HDFS, S3, HBase e.t.c. To make it simple for this PySpark RDD tutorial we are using files from the local system or loading it from the python list to create RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463220ea",
   "metadata": {},
   "source": [
    "###### Create RDD using sparkContext.textFile()\n",
    "\n",
    "Using textFile() method we can read a text (.txt) file into RDD.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2aa92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create RDD from external Data source\n",
    "#rdd2 = spark.sparkContext.textFile(\"/path/textFile.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbedf581",
   "metadata": {},
   "source": [
    "###### Create RDD using sparkContext.wholeTextFiles()\n",
    "\n",
    "wholeTextFiles() function returns a PairRDD with the key being the file path and value being file content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8053755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Reads entire file into a RDD as single record.\n",
    "# rdd3 = spark.sparkContext.wholeTextFiles(\"/path/textFile.txt\")\n",
    "\n",
    "# Besides using text files, we can also create RDD from CSV file, JSON, and more formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b7bb81",
   "metadata": {},
   "source": [
    "###### Create empty RDD using sparkContext.emptyRDD\n",
    "\n",
    "Using ```emptyRDD()``` method on sparkContext we can create an RDD with no data. This method creates an empty RDD with no partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfaf707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates empty RDD with no partition    \n",
    "rdd = spark.sparkContext.emptyRDD \n",
    "# rddString = spark.sparkContext.emptyRDD[String]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465ef333",
   "metadata": {},
   "source": [
    "###### Creating empty RDD with partition\n",
    "\n",
    "Sometimes we may need to write an empty RDD to files by partition, In this case, you should create an empty RDD with partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54f093a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty RDD with partition\n",
    "rdd2 = spark.sparkContext.parallelize([],10) #This creates 10 partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e415ef",
   "metadata": {},
   "source": [
    "###### RDD Parallelize\n",
    "\n",
    "When we use ```parallelize()``` or ```textFile()``` or ```wholeTextFiles()``` methods of SparkContxt to initiate RDD, it automatically splits the data into partitions based on resource availability. when you run it on a laptop it would create partitions as the same number of cores available on your system.\n",
    "\n",
    "**getNumPartitions()** – This a RDD function which returns a number of partitions our dataset split into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfafe31b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'getNumPartitions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial partition count:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNumPartitions\u001b[49m()))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'getNumPartitions'"
     ]
    }
   ],
   "source": [
    "print(\"initial partition count:\"+str(rdd.getNumPartitions()))\n",
    "#Outputs: initial partition count:2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45c02d9",
   "metadata": {},
   "source": [
    "**Set parallelize manually** – We can also set a number of partitions manually, all, we need is, to pass a number of partitions as the second parameter to these functions for example  ```sparkContext.parallelize([1,2,3,4,56,7,8,9,12,3], 10). ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9cb249",
   "metadata": {},
   "source": [
    "###### Repartition and Coalesce\n",
    "\n",
    "Sometimes we may need to repartition the RDD, PySpark provides two ways to repartition; first using ```repartition()``` method which shuffles data from all nodes also called full shuffle and second coalesce() method which shuffle data from minimum nodes, for examples if you have data in 4 partitions and doing ```coalesce(2)``` moves data from just 2 nodes.  \n",
    "\n",
    "Both of the functions take the number of partitions to repartition rdd as shown below.  Note that repartition() method is a very expensive operation as it shuffles data from all nodes in a cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45fd72d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'repartition'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reparRdd \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepartition\u001b[49m(\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mre-partition count:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(reparRdd\u001b[38;5;241m.\u001b[39mgetNumPartitions()))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'repartition'"
     ]
    }
   ],
   "source": [
    "reparRdd = rdd.repartition(4)\n",
    "print(\"re-partition count:\"+str(reparRdd.getNumPartitions()))\n",
    "#Outputs: \"re-partition count:4\n",
    "# Note: repartition() or coalesce() methods also returns a new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf6b969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
