{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24cc8720",
   "metadata": {},
   "source": [
    "###### What is RDD (Resilient Distributed Dataset)?\n",
    "\n",
    "RDD (Resilient Distributed Dataset) is a fundamental building block of PySpark which is fault-tolerant, immutable distributed collections of objects. Immutable meaning once you create an RDD you cannot change it. Each record in RDD is divided into logical partitions, which can be computed on different nodes of the cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f269488a",
   "metadata": {},
   "source": [
    "In other words, RDDs are a collection of objects similar to list in Python, with the difference being RDD is computed on several processes scattered across multiple physical servers also called nodes in a cluster while a Python collection lives and process in just one process.\n",
    "\n",
    "Additionally, RDDs provide data abstraction of partitioning and distribution of the data designed to run computations in parallel on several nodes, while doing transformations on RDD we don’t have to worry about the parallelism as PySpark by default provides.\n",
    "\n",
    "This Apache PySpark RDD tutorial describes the basic operations available on RDDs, such as map(), filter(), and persist() and many more. In addition, this tutorial also explains Pair RDD functions that operate on RDDs of key-value pairs such as groupByKey() and join() etc.\n",
    "\n",
    "Note: RDD’s can have a name and unique identifier (id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1451a0",
   "metadata": {},
   "source": [
    "###### Creating RDD\n",
    "\n",
    "RDD’s are created primarily in two different ways,\n",
    "\n",
    "   * parallelizing an existing collection and\n",
    "   * referencing a dataset in an external storage system (```HDFS```, ```S3``` and many more). \n",
    "\n",
    "Before we look into examples, first let’s initialize SparkSession using the builder pattern method defined in SparkSession class. While initializing, we need to provide the master and application name as shown below. In realtime application, you will pass master from spark-submit instead of hardcoding on Spark application.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a65902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up _JAVA_OPTIONS: -Dawt.useSystemAAFontSettings=on -Dswing.aatext=true\n",
      "Picked up _JAVA_OPTIONS: -Dawt.useSystemAAFontSettings=on -Dswing.aatext=true\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/06 07:46:25 WARN Utils: Your hostname, Jkop resolves to a loopback address: 127.0.1.1; using 172.30.92.4 instead (on interface eth0)\n",
      "22/09/06 07:46:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/06 07:46:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark:SparkSession = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"SparkByExamples.com\") \\\n",
    "      .getOrCreate()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe73f23",
   "metadata": {},
   "source": [
    "```master()``` – If you are running it on the cluster you need to use your master name as an argument to master(). usually, it would be either yarn (Yet Another Resource Negotiator) or mesos depends on your cluster setup.\n",
    "\n",
    "  * Use local[x] when running in Standalone mode. x should be an integer value and should be greater than 0; this represents how many partitions it should create when using RDD, DataFrame, and Dataset. Ideally, x value should be the number of CPU cores you have.\n",
    "\n",
    "```appName()``` – Used to set your application name.\n",
    "\n",
    "```getOrCreate()``` – This returns a SparkSession object if already exists, and creates a new one if not exist.\n",
    "\n",
    "Note: Creating SparkSession object, internally creates one SparkContext per JVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c5f60",
   "metadata": {},
   "source": [
    "###### Create RDD using sparkContext.parallelize()\n",
    "\n",
    "By using ```parallelize()``` function of SparkContext (sparkContext.parallelize() ) you can create an RDD. This function loads the existing collection from your driver program into parallelizing RDD. This is a basic method to create RDD and is used when you already have data in memory that is either loaded from a file or from a database. and it required all data to be present on the driver program prior to creating RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd4eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create RDD from parallelize    \n",
    "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rdd=spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee9050",
   "metadata": {},
   "source": [
    "For production applications, we mostly create RDD by using external storage systems like HDFS, S3, HBase e.t.c. To make it simple for this PySpark RDD tutorial we are using files from the local system or loading it from the python list to create RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463220ea",
   "metadata": {},
   "source": [
    "###### Create RDD using sparkContext.textFile()\n",
    "\n",
    "Using textFile() method we can read a text (.txt) file into RDD.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2aa92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create RDD from external Data source\n",
    "#rdd2 = spark.sparkContext.textFile(\"/path/textFile.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbedf581",
   "metadata": {},
   "source": [
    "###### Create RDD using sparkContext.wholeTextFiles()\n",
    "\n",
    "wholeTextFiles() function returns a PairRDD with the key being the file path and value being file content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8053755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Reads entire file into a RDD as single record.\n",
    "# rdd3 = spark.sparkContext.wholeTextFiles(\"/path/textFile.txt\")\n",
    "\n",
    "# Besides using text files, we can also create RDD from CSV file, JSON, and more formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b7bb81",
   "metadata": {},
   "source": [
    "###### Create empty RDD using sparkContext.emptyRDD\n",
    "\n",
    "Using ```emptyRDD()``` method on sparkContext we can create an RDD with no data. This method creates an empty RDD with no partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfaf707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates empty RDD with no partition    \n",
    "rdd = spark.sparkContext.emptyRDD \n",
    "# rddString = spark.sparkContext.emptyRDD[String]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465ef333",
   "metadata": {},
   "source": [
    "###### Creating empty RDD with partition\n",
    "\n",
    "Sometimes we may need to write an empty RDD to files by partition, In this case, you should create an empty RDD with partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54f093a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty RDD with partition\n",
    "rdd2 = spark.sparkContext.parallelize([],10) #This creates 10 partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e415ef",
   "metadata": {},
   "source": [
    "###### RDD Parallelize\n",
    "\n",
    "When we use ```parallelize()``` or ```textFile()``` or ```wholeTextFiles()``` methods of SparkContxt to initiate RDD, it automatically splits the data into partitions based on resource availability. when you run it on a laptop it would create partitions as the same number of cores available on your system.\n",
    "\n",
    "**getNumPartitions()** – This a RDD function which returns a number of partitions our dataset split into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfafe31b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'getNumPartitions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial partition count:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNumPartitions\u001b[49m()))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'getNumPartitions'"
     ]
    }
   ],
   "source": [
    "print(\"initial partition count:\"+str(rdd.getNumPartitions()))\n",
    "#Outputs: initial partition count:2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45c02d9",
   "metadata": {},
   "source": [
    "**Set parallelize manually** – We can also set a number of partitions manually, all, we need is, to pass a number of partitions as the second parameter to these functions for example  ```sparkContext.parallelize([1,2,3,4,56,7,8,9,12,3], 10). ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9cb249",
   "metadata": {},
   "source": [
    "###### Repartition and Coalesce\n",
    "\n",
    "Sometimes we may need to repartition the RDD, PySpark provides two ways to repartition; first using ```repartition()``` method which shuffles data from all nodes also called full shuffle and second coalesce() method which shuffle data from minimum nodes, for examples if you have data in 4 partitions and doing ```coalesce(2)``` moves data from just 2 nodes.  \n",
    "\n",
    "Both of the functions take the number of partitions to repartition rdd as shown below.  Note that repartition() method is a very expensive operation as it shuffles data from all nodes in a cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45fd72d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'repartition'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reparRdd \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepartition\u001b[49m(\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mre-partition count:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(reparRdd\u001b[38;5;241m.\u001b[39mgetNumPartitions()))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'repartition'"
     ]
    }
   ],
   "source": [
    "reparRdd = rdd.repartition(4)\n",
    "print(\"re-partition count:\"+str(reparRdd.getNumPartitions()))\n",
    "#Outputs: \"re-partition count:4\n",
    "# Note: repartition() or coalesce() methods also returns a new RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df4e7a4",
   "metadata": {},
   "source": [
    "###### PySpark RDD Operations\n",
    "\n",
    "**RDD transformations** – Transformations are lazy operations, instead of updating an RDD, these operations return another RDD.\n",
    "**RDD actions** – operations that trigger computation and return RDD values.\n",
    "\n",
    "###### RDD Transformations with example\n",
    "\n",
    "Transformations on PySpark RDD returns another RDD and transformations are lazy meaning they don’t execute until you call an action on RDD. Some transformations on RDD’s are ```flatMap(),``` ```map(),``` ```reduceByKey(),``` ```filter(),``` ```sortByKey()``` and return new RDD instead of updating the current.\n",
    "\n",
    "In this PySpark RDD Transformation section of the tutorial, I will explain transformations using the word count example. The below image demonstrates different RDD transformations we going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56b16246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an RDD by reading a text file.\n",
    "rdd = spark.sparkContext.textFile(\"doc.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97293ab",
   "metadata": {},
   "source": [
    "**flatMap** – ```flatMap()``` transformation flattens the RDD after applying the function and returns a new RDD. On the below example, first, it splits each record by space in an RDD and finally flattens it. Resulting RDD consists of a single word on each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fe6fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09ec2e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c487003",
   "metadata": {},
   "source": [
    "**map** – ```map()``` transformation is used the apply any complex operations like adding a column, updating a column e.t.c, the output of map transformations would always have the same number of records as input.\n",
    "\n",
    "In our word count example, we are adding a new column with value 1 for each word, the result of the RDD is PairRDDFunctions which contains key-value pairs, word of type String as Key and 1 of type Int as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "404ec67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = rdd2.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322c010",
   "metadata": {},
   "source": [
    "**reduceByKey** – ```reduceByKey()``` merges the values for each key with the function specified. In our example, it reduces the word string by applying the sum function on value. The result of our RDD contains unique words and their count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dff4d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4 = rdd3.reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f087f639",
   "metadata": {},
   "source": [
    "**sortByKey** – ```sortByKey()``` transformation is used to sort RDD elements on key. In our example, first, we convert RDD[(String,Int]) to RDD[(Int, String]) using map transformation and apply sortByKey which ideally does sort on an integer value. And finally, foreach with println statements returns all words in RDD and their count as key-value pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d1933f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 'Project'), (9, 'Gutenberg’s'), (18, 'Alice’s'), (18, 'Adventures'), (18, 'in'), (18, 'Wonderland'), (18, 'by'), (18, 'Lewis'), (18, 'Carroll'), (27, 'This'), (27, 'eBook'), (27, 'is'), (27, 'for'), (27, 'the'), (27, 'use'), (27, 'of'), (27, 'anyone'), (27, 'anywhere'), (27, 'at'), (27, 'no'), (27, 'cost'), (27, 'and'), (27, 'with')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd5 = rdd4.map(lambda x: (x[1],x[0])).sortByKey()\n",
    "#Print rdd5 result to console\n",
    "print(rdd5.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a2f787",
   "metadata": {},
   "source": [
    "**filter** – ```filter()``` transformation is used to filter the records in an RDD. In our example we are filtering all words starts with “a”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92f7af81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(18, 'Wonderland'), (18, 'Carroll'), (27, 'anyone'), (27, 'anywhere'), (27, 'at'), (27, 'and')]\n"
     ]
    }
   ],
   "source": [
    "rdd6 = rdd5.filter(lambda x : 'a' in x[1])\n",
    "print(rdd6.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e317fe89",
   "metadata": {},
   "source": [
    "**RDD Actions with example**\n",
    "\n",
    "RDD Action operations return the values from an RDD to a driver program. In other words, any RDD function that returns non-RDD is considered as an action. \n",
    "\n",
    "In this section of the PySpark RDD tutorial, we will continue to use our word count example and performs some actions on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eb3fd5",
   "metadata": {},
   "source": [
    "``count()`` – Returns the number of records in an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62af3d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count : 6\n"
     ]
    }
   ],
   "source": [
    "# Action - count\n",
    "print(\"Count : \"+str(rdd6.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7eb8d",
   "metadata": {},
   "source": [
    "```first()``` – Returns the first record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef41a230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Record : 18,Wonderland\n"
     ]
    }
   ],
   "source": [
    "# Action - first\n",
    "firstRec = rdd6.first()\n",
    "print(\"First Record : \"+str(firstRec[0]) + \",\"+ firstRec[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb869f70",
   "metadata": {},
   "source": [
    "```max()``` – Returns max record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19bba9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Record : 27,at\n"
     ]
    }
   ],
   "source": [
    "# Action - max\n",
    "datMax = rdd6.max()\n",
    "print(\"Max Record : \"+str(datMax[0]) + \",\"+ datMax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc7183",
   "metadata": {},
   "source": [
    "```reduce()``` – Reduces the records to single, we can use this to count or sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56ff1d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataReduce Record : 144\n"
     ]
    }
   ],
   "source": [
    "# Action - reduce\n",
    "totalWordCount = rdd6.reduce(lambda a,b: (a[0]+b[0],a[1]))\n",
    "print(\"dataReduce Record : \"+str(totalWordCount[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1112d69d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
